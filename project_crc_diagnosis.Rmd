---
title: "A Statistical approach to Colorectal Cancer diagnosis based on Protein signature"
author: "Group 5"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/home/abhijeet/Documents/CS 7280/project")
seed.id <- 42
library(Amelia)
library(caret)
library(corrplot)
library(ResourceSelection)
library(faraway)
library(ROCR)
library(gridExtra)
vif_func<-function(in_frame,thresh=10,trace=T,...){
  
  require(fmsb)
  
  if(class(in_frame) != 'data.frame') in_frame<-data.frame(in_frame)
  
  #get initial vif value for all comparisons of variables
  vif_init<-NULL
  var_names <- names(in_frame)
  for(val in var_names){
    regressors <- var_names[-which(var_names == val)]
    form <- paste(regressors, collapse = '+')
    form_in <- formula(paste(val, '~', form))
    vif_init<-rbind(vif_init, c(val, VIF(lm(form_in, data = in_frame, ...))))
  }
  vif_max<-max(as.numeric(vif_init[,2]))
  
  if(vif_max < thresh){
    if(trace==T){ #print output of each iteration
      prmatrix(vif_init,collab=c('var','vif'),rowlab=rep('',nrow(vif_init)),quote=F)
      cat('\n')
      cat(paste('All variables have VIF < ', thresh,', max VIF ',round(vif_max,2), sep=''),'\n\n')
    }
    return(var_names)
  }
  else{
    
    in_dat<-in_frame
    
    #backwards selection of explanatory variables, stops when all VIF values are below 'thresh'
    while(vif_max >= thresh){
      
      vif_vals<-NULL
      var_names <- names(in_dat)
      
      for(val in var_names){
        regressors <- var_names[-which(var_names == val)]
        form <- paste(regressors, collapse = '+')
        form_in <- formula(paste(val, '~', form))
        vif_add<-VIF(lm(form_in, data = in_dat, ...))
        vif_vals<-rbind(vif_vals,c(val,vif_add))
      }
      max_row<-which(vif_vals[,2] == max(as.numeric(vif_vals[,2])))[1]
      
      vif_max<-as.numeric(vif_vals[max_row,2])
      
      if(vif_max<thresh) break
      
      if(trace==T){ #print output of each iteration
        prmatrix(vif_vals,collab=c('var','vif'),rowlab=rep('',nrow(vif_vals)),quote=F)
        cat('\n')
        cat('removed: ',vif_vals[max_row,1],vif_max,'\n\n')
        flush.console()
      }
      
      in_dat<-in_dat[,!names(in_dat) %in% vif_vals[max_row,1]]
      
    }
    
    return(names(in_dat))
    
  }
  
}
```

# Pre-Processing: Cleaning and Formatting Data
## Input
```{r}
load(file = "Surinova_training_abun.Rda")
cols <- Surinova_training_abun[,1]
input.train.raw <- t(Surinova_training_abun[,-1])
input.train.raw <- data.frame(input.train.raw)
colnames(input.train.raw) <- cols

load(file = "Surinova_testing_abun.Rda")
cols <- Surinova_testing_abun[,1]
input.test.raw <- t(Surinova_testing_abun[,-1])
input.test.raw <- data.frame(input.test.raw)
colnames(input.test.raw) <- cols

# check to see the column names match in both the input datasets.
which(colnames(input.train.raw) != colnames(input.test.raw))

# Sicne protein abundances are normalized within each dataset and not between training and validation set, we cannot merge these both datasets.

# Formatting Column Names
colnames(input.train.raw)[colnames(input.train.raw) == 'AIAG-Bovine'] <- 'AIAG.Bovine'
colnames(input.train.raw)[colnames(input.train.raw) == 'FETUA-Bovine'] <- 'FETUA.Bovine'

colnames(input.test.raw)[colnames(input.test.raw) == 'AIAG-Bovine'] <- 'AIAG.Bovine'
colnames(input.test.raw)[colnames(input.test.raw) == 'FETUA-Bovine'] <- 'FETUA.Bovine'

# Create Class label: CRC = 1, Healthy = 0
input.train.raw$Class <- gsub("_.*", "\\1",rownames(input.train.raw))
input.train.raw$Class <- as.factor(input.train.raw$Class)
table(input.train.raw$Class)

input.test.raw$Class <- gsub("_.*", "\\1",rownames(input.test.raw))
input.test.raw$Class[input.test.raw$Class == 'Control'] <- 'Healthy'
input.test.raw$Class <- as.factor(input.test.raw$Class)
table(input.test.raw$Class)


head(input.train.raw)
str(input.train.raw)
```
There are 200 subjects and 72 proteins in the training dataset out of which 100 subjects are diagnosed with CRC and 100 are healthy.   
There are 269 subjects and 72 proteins in the testing dataset out of which 202 subjects are diagnosed with CRC and 67 are healthy.   

# Pre-Processing: Missing Values
## Visualizing Missing Values in Training dataset
```{r}
par(mfrow=c(1,1))
missmap(input.train.raw, main = "Missing values vs observed for Raw Training Dataset", rank.order=FALSE)
```

## Visualizing Missing Values in Testing dataset
```{r}
missmap(input.test.raw, main = "Missing values vs observed for Raw Test Dataset", rank.order=FALSE)
```
Most of the missing values are related to the second dataset("Surinova_testing_abun.Rda")

## Dealing with Missing Values Pt. 1: Drop > 25% Missing
### Training Dataset
```{r}
# Remove Columns with Missing Values.
missing.percent.train <- sapply(input.train.raw,function(x) sum(is.na(x)) * 100/length(x))
missing.columns.train <- sort(missing.percent.train[missing.percent.train != 0])
missing.columns.train
length(missing.columns.train)
```
There are 6 columns with Missing Values in the Training Dataset. 

### Testing Dataset
```{r}
# Remove Columns with Missing Values.
missing.percent.test <- sapply(input.test.raw, function(x) sum(is.na(x)) * 100/length(x))
missing.columns.test <- sort(missing.percent.test[missing.percent.test != 0])
missing.columns.test
length(missing.columns.test)
```
There are 34 columns with Missing Values in the Testing Dataset.  

As we observe from the above tables, there are large number of columns with more than 25 % missing data.We'll drop these columns from both the datasets.
```{r}
# we'll combine the dropped column list from both training and testing datasets to keep the remaining columns consistent.
dropped.columns.train <- names(missing.columns.train[missing.columns.train > 25])
dropped.columns.test <- names(missing.columns.test[missing.columns.test > 25])
dropped.columns.final <- union(dropped.columns.train, dropped.columns.test)
dropped.columns.final

input.train.df <- input.train.raw
input.test.df <- input.test.raw
input.train.df[, dropped.columns.final] <- NULL
input.test.df[, dropped.columns.final] <- NULL
dim(input.train.df)
dim(input.test.df)
```
In total, We dropped 17 columns.  

## Dealing with Missing Values Pt. 2: Replace with Min
For the rest of the columns with missing values, we'll replace the missing values with the mean.
```{r}
# Replace Missing Values with Min
replace.min <- function(x) replace(x, is.na(x), min(x, na.rm=TRUE))
cols <- colnames(subset(input.train.df, select = -Class))
input.train.df[, cols] <- sapply(input.train.df[, cols], replace.min)
input.test.df[, cols] <- sapply(input.test.df[, cols], replace.min)
```

Now, we do not have any missing values in our data.   

# Data Exploration
## Boxplot
```{r}
par(mfrow = c(1,1))
boxplot(input.train.df)
boxplot(input.test.df)
```
The boxplots does show some points as outliers but since they are all logarithmic values, scale is small.
Also, we confirm that the data has indeed been normalized separately for each dataset.  

## Correlation amongst Predictors.
```{r}
# Correlation between Predictors
correlations <- cor(subset(input.train.df, select=-Class))
png('correlations.png', width=4, height=4, units="in", res=300)
corrplot(correlations, method="square", tl.cex = 0.5)
#dev.off()
#dev.off()

cols <- colnames(subset(input.train.df, select = -Class))
keep.cols <- vif_func(in_frame=input.train.df[, cols],thresh=5,trace=F)
keep.cols
input.train.df <- input.train.df[, c(keep.cols, "Class")]
input.test.df <- input.test.df[, c(keep.cols, "Class")]
dim(input.train.df)
dim(input.test.df)
```

Blue represents positive correlation and Red negative. The larger the squares, the larger the correlation.  
We observe that some of the predictors are highly correlated with each other.  

# Preprocessing: Shuffling and Splitting the Datasets
The input datasets consisted of all CRC subject data in the to half and Healthy subject data in the bottom half. The datasets are therefore shuffled to prevent bias in the training and testing cohorts.  We'll split the training dataset into 2 subsets: training(80%) and validation sets(20%).  We'll not split the testing dataset.  
```{r}
set.seed(seed.id)
training <- input.train.df
inTrain <- createDataPartition(training$Class, p = 0.8, list=FALSE)
train <- training[inTrain, ]
validation <- training[-inTrain, ]
  
set.seed(seed.id)
testing <- input.test.df[sample(nrow(input.test.df)), ]
```

# Model Selection
```{r}
glm.full <- glm(Class ~ ., data = train, family = binomial)
glm.null <- glm(Class ~ 1, data = train, family = binomial)
```

## AIC-based backward selection
```{r}
model.aic.backward <- step(glm.full, direction = "backward", trace = 1)
summary(model.aic.backward)
```
This results in 9 columns in the best model with AIC = 194.09

## AIC-based forward selection
```{r}
model.aic.forward <- step(glm.null, direction = "forward", trace = 1, scope = list(lower=glm.null, upper=glm.full))
summary(model.aic.forward)
```
This results in 7 columns in the best model with AIC = 194.47

## AIC-based forward-backward selection
```{r}
model.aic.both <- step(glm.null, direction = "both", trace = 1,  scope=list(lower=glm.null, upper=glm.full))
summary(model.aic.both)
```
This results in 7 columns in the model with AIC = 194.19  

## BIC-based forward-backward selection
```{r}
model.sbc.both <- step(glm.full, direction = "both", trace = 1,  scope=list(lower=glm.null, upper=glm.full),
                       k=log(nrow(train)))
summary(model.sbc.both)
```
As expected, the BIC selection results in a parsimonious model with 5 columns with SBC = 213.11  

We select the top 2 models in each of the above selections as our candidate models.
```{r}
formula.BestAICBackward1 <- model.aic.backward$formula
formula.BestAICBackward2 <- as.formula(Class ~ FCGBP + FETUA.Bovine + FHR3 + HP + IGHG2 + KNG1 + LAMP2 + LUM + MMRN1 + SERPINA7)

formula.BestAICForward1 <- model.aic.forward$formula
formula.BestAICForward2 <- as.formula(Class ~ TIMP1 + LAMP2 + HP + LRG1 + SERPINA7 + LUM)

formula.BestAICBoth1 <- model.aic.both$formula
formula.BestAICBoth2 <- as.formula(Class ~  TIMP1 + LAMP2 + LRG1 + SERPINA7 + LUM + NCAM1)

formula.BestSBCBoth1 <- model.sbc.both$formula
formula.BestSBCBoth2 <- as.formula(Class ~ FHR3 + HP + LAMP2 + LUM + MMRN1 + SERPINA7)

# Plot the Candidate Models' AIC Values
candidateModels.aic <- data.frame(model <- c("BestAICBackward1", "BestAICBackward2", "BestAICForward1", "BestAICForward2", "BestAICBoth1", "BestAICBoth2", "BestSBCBoth1", "BestSBCBoth2"), AIC <- c(model.aic.backward$aic, 194.97, model.aic.forward$aic, 194.75, model.aic.both$aic, 194.19, 213.11, 215.21))
colnames(candidateModels.aic) <- c("Model", "AIC")
ggplot(candidateModels.aic, aes(x=Model, y=AIC)) + geom_point() + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + ggtitle("Model vs AIC")
```
We can ignore the AIC values for BestSBCBoth1 and BestSBCBoth2 in the above plot as it considers the SBC criterion instead. 

# Candidate Model Fitting
## Train Logistic Regression on the candidate Models
```{r}
model.BestAICBackward1 <- glm(formula.BestAICBackward1, family=binomial(link='logit'), data=train)
summary(model.BestAICBackward1)

model.BestAICBackward2 <- glm(formula.BestAICBackward2, family=binomial(link='logit'), data=train)
summary(model.BestAICBackward2)

model.BestAICForward1 <- glm(formula.BestAICForward1, family=binomial(link='logit'), data=train)
summary(model.BestAICForward1)

model.BestAICForward2 <- glm(formula.BestAICForward2, family=binomial(link='logit'), data=train)
summary(model.BestAICForward2)

model.BestAICBoth1 <- glm(formula.BestAICBoth1, family=binomial(link='logit'), data=train)
summary(model.BestAICBoth1)

model.BestAICBoth2 <- glm(formula.BestAICBoth2, family=binomial(link='logit'), data=train)
summary(model.BestAICBoth2)

model.BestSBCBoth1 <- glm(formula.BestSBCBoth1, family=binomial(link='logit'), data=train)
summary(model.BestSBCBoth1)

model.BestSBCBoth2 <- glm(formula.BestSBCBoth2, family=binomial(link='logit'), data=train)
summary(model.BestSBCBoth2)
```

# Model Evaluation
## Goodness of Fit: Homer Lemeshow Test
We consider the model to be unfit if p < 0.05.  
```{r}
# Converting Factor label to Numeric, "CRC" -> 1, "Healthy" -> 0
train.numeric <- train
train.numeric$Class <- as.numeric(train$Class)
train.numeric$Class[train.numeric$Class == 1] <- 0
train.numeric$Class[train.numeric$Class == 2] <- 1

res <- hoslem.test(train.numeric$Class, fitted(model.BestAICBackward1), g=10)
res
```
The p-value is `r res$p.value` so there's no evidence the model in incorrect.     

```{r}
res <- hoslem.test(train.numeric$Class, fitted(model.BestAICBackward2), g=10)
res
```
The p-value is `r res$p.value` so there's no evidence the model in incorrect.     

```{r}
res <- hoslem.test(train.numeric$Class, fitted(model.BestAICForward1), g=10)
res
```
The p-value is `r res$p.value` so there's no evidence the model in incorrect.     

```{r}
res <- hoslem.test(train.numeric$Class, fitted(model.BestAICForward2), g=10)
res
```
The p-value is `r res$p.value` so there's no evidence the model in incorrect.     

```{r}
res <- hoslem.test(train.numeric$Class, fitted(model.BestAICBoth1), g=10)
res
```
The p-value is `r res$p.value` so there's no evidence the model in incorrect.     

```{r}
res <- hoslem.test(train.numeric$Class, fitted(model.BestAICBoth2), g=10)
res
```
The p-value is `r res$p.value` so there's no evidence the model in incorrect.     

```{r}
res <- hoslem.test(train.numeric$Class, fitted(model.BestSBCBoth1), g=10)
res
```
The p-value is `r res$p.value` so there's no evidence the model in incorrect.     

```{r}
res <- hoslem.test(train.numeric$Class, fitted(model.BestSBCBoth2), g=10)
res
```
The p-value is `r res$p.value` so there's no evidence the model in incorrect.     

From the above tests, we do not eliminate any Model from the candidate models.  

# Diagnostic Residual Plots
## Residuals with Predicted Probabilities with Lowess Smooth  

If the model is correct, a lowess smooth of the plot of the residuals against the estimated probability $\hat{\pi}_i$ should result approximately in a horizontal line with zero intercept.

```{r}
png('residuals_lowess.png')
par(mfrow=c(3,3))
scatter.smooth(predict(model.BestAICBackward1, type = "response"), 
               residuals(model.BestAICBackward1), xlab = "Estimated Probability", ylab = "Deviance Residual", main="Model 1")

scatter.smooth(predict(model.BestAICBackward2, type = "response"), 
               residuals(model.BestAICBackward2), xlab = "Estimated Probability", ylab = "Deviance Residual", main="Model 2")

scatter.smooth(predict(model.BestAICForward1, type = "response"), 
               residuals(model.BestAICForward1), xlab = "Estimated Probability", ylab = "Deviance Residual", main="Model 3")

scatter.smooth(predict(model.BestAICForward2, type = "response"), 
               residuals(model.BestAICForward2), xlab = "Estimated Probability", ylab = "Deviance Residual", main="Model 4")

scatter.smooth(predict(model.BestAICBoth1, type = "response"), 
               residuals(model.BestAICBoth1), xlab = "Estimated Probability", ylab = "Deviance Residual", main="Model 5")

scatter.smooth(predict(model.BestAICBoth2, type = "response"), 
               residuals(model.BestAICBoth2), xlab = "Estimated Probability", ylab = "Deviance Residual", main="Model 6")

scatter.smooth(predict(model.BestSBCBoth1, type = "response"), 
               residuals(model.BestSBCBoth1), xlab = "Estimated Probability", ylab = "Deviance Residual", main="Model 7")

scatter.smooth(predict(model.BestSBCBoth2, type = "response"), 
               residuals(model.BestSBCBoth2), xlab = "Estimated Probability", ylab = "Deviance Residual", main="Model 8")
#dev.off()
```
All the Plots look good.  

# Half-Normal Probability Plot  
A half-normal probability plot helps to highlight outlying deviance residuals even though the residuals are not normally distributed. Outliers will appear at the top right of a half-normal probability plot as points separated from the others.  
```{r}
png('half_normal.png')
par(mfrow=c(3,3))
halfnorm(residuals(model.BestAICBackward1), main="Model 1")
halfnorm(residuals(model.BestAICBackward2), main="Model 2")
halfnorm(residuals(model.BestAICForward1), main="Model 3")
halfnorm(residuals(model.BestAICForward2), main="Model 4")
halfnorm(residuals(model.BestAICBoth1), main="Model 5")
halfnorm(residuals(model.BestAICBoth2), main="Model 6")
halfnorm(residuals(model.BestSBCBoth1), main="Model 7")
halfnorm(residuals(model.BestSBCBoth2), main="Model 8")
#dev.off()
```
We observe from the above plots that observation 18, 40 and 112 might be an outlier.  

# Overdispersion
Sometimes we can get a deviance that is much larger than expected if the model was correct. It can be due to the presence of outliers, sparse data or clustering of data. The approach to deal with overdispersion is to add a dispersion parameter $\sigma^2$ . It can be estimated with: $\hat{\sigma}^2 = \frac{\chi^2}{n - p}$  (p = no. of parameters in model).  
$Var \{ Y_i \} = \phi n_i \pi_i \{ 1 - \pi_i \}$
We consider overdispersion to exist if $\phi$ >> 1.  
```{r}
par(mfrow=c(1,1))
# No. of observations in the training dataset.
n <- nrow(train)
# No. of parameters in the model.
p <- length(model.BestAICBackward1$coefficients)
phi <- sum(residuals(model.BestAICBackward1, type = "pearson")^2) / (n - p)
phi
```
The dispersion parameter is not very different than one (no dispersion).  

```{r}
p <- length(model.BestAICBackward2$coefficients)
phi <- sum(residuals(model.BestAICBackward2, type = "pearson")^2) / (n - p)
phi
```
The dispersion parameter is not very different than one (no dispersion).  

```{r}
p <- length(model.BestAICForward1$coefficients)
phi <- sum(residuals(model.BestAICForward1, type = "pearson")^2) / (n - p)
phi
```
The dispersion parameter is not very different than one (no dispersion).  

```{r}
p <- length(model.BestAICForward2$coefficients)
phi <- sum(residuals(model.BestAICForward2, type = "pearson")^2) / (n - p)
phi
```
The dispersion parameter is not very different than one (no dispersion).  

```{r}
p <- length(model.BestAICBoth1$coefficients)
phi <- sum(residuals(model.BestAICBoth1, type = "pearson")^2) / (n - p)
phi
```
The dispersion parameter is not very different than one (no dispersion).  

```{r}
p <- length(model.BestAICBoth2$coefficients)
phi <- sum(residuals(model.BestAICBoth2, type = "pearson")^2) / (n - p)
phi
```
The dispersion parameter is not very different than one (no dispersion).  

```{r}
p <- length(model.BestSBCBoth1$coefficients)
phi <- sum(residuals(model.BestSBCBoth1, type = "pearson")^2) / (n - p)
phi
```
The dispersion parameter is not very different than one (no dispersion).  

```{r}
p <- length(model.BestSBCBoth2$coefficients)
phi <- sum(residuals(model.BestSBCBoth2, type = "pearson")^2) / (n - p)
phi
```
The dispersion parameter is not very different than one (no dispersion).   

# Predictive Ability of the Model   
```{r}
# Divide training into 10 equal parts, keep one part as validation set and rest as training.
model.list <- list()
model.list[["BestAICBackward1"]] <- formula.BestAICBackward1
model.list[["BestAICBackward2"]] <- formula.BestAICBackward2
model.list[["BestAICForward1"]] <- formula.BestAICForward1
model.list[["BestAICForward2"]] <- formula.BestAICForward2
model.list[["BestAICBoth1"]] <- formula.BestAICBoth1
model.list[["BestAICBoth2"]] <- formula.BestAICBoth2
model.list[["BestSBCBoth1"]] <- formula.BestSBCBoth1
model.list[["BestSBCBoth2"]] <- formula.BestSBCBoth2

k = 1
roc.mat <- matrix(list(), nrow=length(model.list), ncol=k)
fpr.mat <- matrix(list(), nrow=length(model.list), ncol=k)
tpr.mat <- matrix(list(), nrow=length(model.list), ncol=k)
auc.mat <- matrix(numeric(), nrow=length(model.list), ncol=k)
roc.plots.mat <- matrix(list(), nrow=length(model.list), ncol=k)
#png("ROC_plots.png")
par(mfrow=c(3,3))
for (model.id in 1:length(model.list)){
  cv.id <- 1
  fit <- glm(model.list[[model.id]], data=train, family=binomial(link='logit'))
  predicted <- predict(fit, newdata=subset(validation, select=-Class))
  prob <- prediction(predicted, validation$Class)
  tprfpr <- performance(prob, "tpr", "fpr")
  tpr <- unlist(slot(tprfpr, "y.values"))
  fpr <- unlist(slot(tprfpr, "x.values"))
  roc <- data.frame(tpr, fpr)
  auc <- performance(prob, measure = "auc")
  auc <- auc@y.values[[1]]
  roc.mat[[model.id, cv.id]] <- roc
  fpr.mat[[model.id, cv.id]] <- fpr
  tpr.mat[[model.id, cv.id]] <- tpr
  auc.mat[[model.id, cv.id]] <- auc
  roc.plots.mat[[model.id, cv.id]] <- ggplot(roc) + geom_line(aes(x = fpr, y = tpr)) + geom_abline(intercept = 0, slope = 1, colour = "gray") +  ylab("Sensitivity") + xlab("1 - Specificity") + ggtitle(paste0("Model ",model.id, ",AUC: " ,round(auc, 4)))
}
do.call(grid.arrange, roc.plots.mat[, 1])
#dev.off()
mean.auc <- apply(auc.mat, 1, mean)
names(mean.auc) <- names(model.list)
mean.auc
#best.model.id <- 7
best.model.id <- which.max(mean.auc)
names(model.list)[best.model.id]
```
The area under the function provides an unbiased, and non-parametric measure of the discrimination ability of the model. 
AOC = 0.5 means that predictions are no better than random guessing. An AUROC value >= 0.80 is considered ideal.  
The Candidate model with best AUROC value is "BestAICForward2" with value 0.7925. We consider this as our final model.  

## Selecting the best Cutoff
```{r}
par(mfrow=c(1,1))
set.seed(seed.id)
fit <- train(model.list[[best.model.id]], data = train, method = "glm", family="binomial")
predicted <- predict(fit, newdata=subset(validation, select=-Class), type="prob", dispersion = 1.129278)[,1]

cutoffs <- seq(0, 1, 0.1)
sens <- c()
spec <- c()
acc <- c()
for (cutoff in cutoffs){
  pred <- ifelse(predicted >= cutoff, "CRC","Healthy")
  cm <- confusionMatrix(pred, validation$Class)
  sens <- c(sens, cm$byClass[["Sensitivity"]])
  spec <- c(spec, cm$byClass[["Specificity"]])
  acc <- c(acc, cm$byClass[["Balanced Accuracy"]])
}
df <- data.frame(cutoffs <- cutoffs, sens <- sens, spec <- spec, acc <- acc)
names(df) <- c("cutoffs", "sens", "spec", "acc")
df

png("sens_spec_acc.png")
plot(round(df$cutoffs, 4),df$sens, xlab="Cutoff", ylab="Value",cex.lab=1.5,cex.axis=1.5,ylim=c(0,1),type="l",lty=2,lwd=2,axes=TRUE)
lines(round(df$cutoffs, 4), df$spec,lty=4,lwd=3)
lines(round(df$cutoffs, 4), df$acc,lwd=2, type="l")
legend(0.5,.25,lty=c(2,4,1),lwd=c(2,3,2),c("Sensitivity","Specificity","Accuracy"))
#dev.off()

pred <- ifelse(predicted >= 0.45, "CRC","Healthy")
confusionMatrix(pred, validation$Class)
```


## Fitting the Best Model
We now fit the Best Model with the full training dataset.
```{r}
set.seed(seed.id)
best.fit <- train(model.list[[best.model.id]], data = training, method = "glm", family="quasibinomial")
summary(best.fit)

# Coefficients
exp(coef(best.fit$finalModel))

# Confidence Interval of Coefficients
confint(best.fit$finalModel)
```

## Plotting Predicted Probability Lines
### CP
```{r}
training.numeric <- training
training.numeric$Class <- as.numeric(training$Class)
training.numeric$Class[training.numeric$Class == 1] <- 0
training.numeric$Class[training.numeric$Class == 2] <- 1

png("predicted_prob.png")
par(mfrow=c(3,2))
test <- with(training, data.frame(TIMP1=TIMP1, LAMP2=median(LAMP2), HP=median(HP), LRG1=median(LRG1), SERPINA7= median(SERPINA7), LUM=median(LUM)))
test$P <- predict(best.fit, newdata=test, type='prob')[, 1]
plot(training$TIMP1, training.numeric$Class, pch=16, xlab="TIMP1", ylab="Class")
points(training$TIMP1, test$P)

test <- with(training, data.frame(TIMP1=median(TIMP1), LAMP2=LAMP2, HP=median(HP), LRG1=median(LRG1), SERPINA7= median(SERPINA7), LUM=median(LUM)))
test$P <- predict(best.fit, newdata=test, type='prob')[, 1]
plot(training$LAMP2, training.numeric$Class, pch=16, xlab="LAMP2", ylab="Class")
points(training$LAMP2, test$P)

test <- with(training, data.frame(TIMP1=median(TIMP1), LAMP2=median(LAMP2), HP=HP, LRG1=median(LRG1), SERPINA7= median(SERPINA7), LUM=median(LUM)))
test$P <- predict(best.fit, newdata=test, type='prob')[, 1]
plot(training$HP, training.numeric$Class, pch=16, xlab="HP", ylab="Class")
points(training$HP, test$P)

test <- with(training, data.frame(TIMP1=median(TIMP1), LAMP2=median(LAMP2), HP=median(HP), LRG1=LRG1, SERPINA7= median(SERPINA7), LUM=median(LUM)))
test$P <- predict(best.fit, newdata=test, type='prob')[, 1]
plot(training$LRG1, training.numeric$Class, pch=16, xlab="LRG1", ylab="Class")
points(training$LRG1, test$P)

test <- with(training, data.frame(TIMP1=median(TIMP1), LAMP2=median(LAMP2), HP=median(HP), LRG1=median(LRG1), SERPINA7=SERPINA7, LUM=median(LUM)))
test$P <- predict(best.fit, newdata=test, type='prob')[, 1]
plot(training$SERPINA7, training.numeric$Class, pch=16, xlab="SERPINA7", ylab="Class")
points(training$SERPINA7, test$P)

test <- with(training, data.frame(TIMP1=median(TIMP1), LAMP2=median(LAMP2), HP=median(HP), LRG1=median(LRG1), SERPINA7=median(SERPINA7), LUM=LUM))
test$P <- predict(best.fit, newdata=test, type='prob')[, 1]
plot(training$LUM, training.numeric$Class, pch=16, xlab="LUM", ylab="Class")
points(training$LUM, test$P)
#dev.off()
par(mfrow=c(1,1))
```
## Test the Final Model
We now introduce the test dataset for final results.
```{r}
# Confusion Matrix for Training dataset
predicted <- predict(best.fit, newdata=subset(training, select=-Class), type="prob", dispersion = 1.129278)[,1]
pred <- ifelse(predicted >= 0.45, "CRC", "Healthy")
confusionMatrix(data=pred, training$Class)

# Confusion Matrix for Testing dataset
predicted <- predict(fit, newdata=subset(testing, select=-Class), type="prob", dispersion = 1.129278)[,1]
pred <- ifelse(predicted >= 0.45, "CRC","Healthy")
confusionMatrix(data=pred, testing$Class)
```
The Balanced Accuracy is almost similar for both training and testing datasets.
